# Dissertation-Project

FNTP: Fake News Thesis Project
This README entails the specifics of each notebook and the recommended order to go through them in line with the methodology in the written thesis.

¬LSTM-Bisaillon-dataset: LSTM model run on CPU obtaining accuracy from the Bisaillon dataset.

¬LSTM-Lifferth-dataset: LSTM model run on CPU obtaining accuracy from the Lifferth dataset.

¬CHATGPT3.5_Outputs: Analysis of 15 prompts used in CHATGPT3.5 online. 

¬Google_BARD_Outputs: Analysis of 15 prompts used in Google BARD online.

¬HuggingChat_Outputs: Analysis of 15 prompts used in HuggingChat online.

¬BERT-Bisaillon-dataset-TEXT: BERT model partially run on GoogleColab GPU with the full body of text from the Bisaillon dataset.

¬BERT-Bisaillon-dataset-HEADLINES: BERT model run on GoogleColab GPU and fine-tuned with the titles from Bisaillon dataset.

¬BERT-Lifferth-dataset-HEADLINES: BERT model run on GoogleColab GPU and fine-tuned with the titles from the Lifferth dataset.

¬roBERTa-Bisaillon-dataset-HEADLINES: RoBERTa model run on CPU, already pretrained on the Bisaillon dataset. Implemented directly from the transformers library. 

¬AI-and-human-analyis: analysis from the full data post the study. This is broken down into a section for AI and a section for humans. The subdivide further to analyse the results in comparison to each LLM and contextual understanding. 

¬human-csv-files: contains the task sheet used for human analysis. There are 3 versions of this file - a blank one with all the headlines produced by the LLMs, a shuffled blank one which was given to the participants of the research study and a full data version post the study. There are also tables created with results from the study analysis.

EXTRA:
¬DistillBERT-language-analysis: Experimenting with DistillBERT model direct from the transformers library to see how language is understood by the model.

 











